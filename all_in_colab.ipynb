{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EB2UlyxTLhBW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dz3ho7FLKLB",
        "outputId": "2b14d03b-31cc-43f5-e3f6-f8445aa5e512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import random\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "print(tf.__version__)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Symtom Model"
      ],
      "metadata": {
        "id": "EB2UlyxTLhBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Find mohamed_tokenizer.pickle ,mohamed_tokenizer.pickle\n",
        "in symtom folder to load both of them </h1>"
      ],
      "metadata": {
        "id": "wf7XqJEASc-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_symp = ['Acne', 'Arthritis', 'Bronchial Asthma', 'Cervical spondylosis',\n",
        "       'Chicken pox', 'Common Cold', 'Dengue', 'Dimorphic Hemorrhoids',\n",
        "       'Fungal infection', 'Hypertension', 'Impetigo', 'Jaundice',\n",
        "       'Malaria', 'Migraine', 'Pneumonia', 'Psoriasis', 'Typhoid',\n",
        "       'Varicose Veins', 'allergy', 'diabetes', 'drug reaction',\n",
        "       'gastroesophageal reflux disease', 'peptic ulcer disease',\n",
        "       'urinary tract infection']\n",
        "\n",
        "try:\n",
        "    with open('/content/mohamed_tokenizer.pickle', 'rb') as t: ## upload mohamed_tokenizer.pickle and put the path here\n",
        "        tokenizer = pickle.load(t)\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "model_symp = tf.keras.models.load_model('/content/mohamed_symptom.h5') ## upload mohamed_symptom.h5 and put the path here\n",
        "def input_tokenizer(txt,tok):\n",
        "  val=tok.texts_to_sequences(txt)\n",
        "  p =pad_sequences(val,maxlen=43,padding='post',truncating='post')\n",
        "  return p\n",
        "def prediction_symptom(txt,classes,model):\n",
        "  output=model.predict(txt)\n",
        "  pred = np.argmax(output,axis=1)\n",
        "  output_pred=classes[pred[0]]\n",
        "  return output_pred"
      ],
      "metadata": {
        "id": "ieEcHHhcLZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  txt=input('Enter what is your feeling:')\n",
        "  p=input_tokenizer(txt,tokenizer)\n",
        "  output=prediction_symptom(p,all_symp,model_symp)\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "GikcZa2dLmVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heart Desease"
      ],
      "metadata": {
        "id": "8W-K70A-M7XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Find model_heart.pkl\n",
        "in heart folder to load it </h1>"
      ],
      "metadata": {
        "id": "xx7_am8hSwdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/model_heart.pkl','rb') as m: ## upload model_heart and put the path here\n",
        "    model_heart =pickle.load(m)\n"
      ],
      "metadata": {
        "id": "hvhljrexMz7g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example to test from dataset"
      ],
      "metadata": {
        "id": "Gs4CAuIbLSGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr1 = np.array([[ 61.,   1.,   4., 160.,   0.,   1.,   1., 145.,   0.,   1.,   2.]]) ### OUTPUT = 1\n",
        "arr2 = np.array([[43. , 0., 2., 150., 186., 0., 0., 154., 0., 0., 1.]]) ### OUTPUT = 0"
      ],
      "metadata": {
        "id": "-0vVM6yRKWyq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=model_heart.predict(arr1)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDOyDAC0KuKA",
        "outputId": "cb4e9b49-e9fc-4cfe-a038-f566c5b83465"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run this function if you want to input yourself the data the sex input should be of type string (male or female)**"
      ],
      "metadata": {
        "id": "oEH4ggZlLbee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_pred_heart():\n",
        "  age=float(input('age: '))\n",
        "  sex=str(input('sex: '))\n",
        "  chest_pain=float(input('chest pain: '))\n",
        "  resting_bp_s=float(input('resting_bp_s: '))\n",
        "  cholesterol=float(input('cholesterol: '))\n",
        "  fasting_blood_sugar=float(input('fasting blood sugar: '))\n",
        "  resting_ecg=float(input('resting ecg: '))\n",
        "  max_heart_rate=float(input('max heart rate: '))\n",
        "  exercise_angina=float(input('exercise angina: '))\n",
        "  oldpeak=float(input('oldpeak: '))\n",
        "  ST_slope=float(input('ST slope: '))\n",
        "  if sex.lower=='male':\n",
        "    sex=float(1)\n",
        "  else:\n",
        "    sex=float(0)\n",
        "  data=np.array([age,sex,chest_pain,resting_bp_s,cholesterol,fasting_blood_sugar,resting_ecg,max_heart_rate,exercise_angina,oldpeak,ST_slope])\n",
        "  data=np.expand_dims(data,axis=0)\n",
        "  print(data.shape)\n",
        "  pred = model_heart.predict(data)\n",
        "  s=\"Can't predict\"\n",
        "  if pred[0]==1:\n",
        "    s=\"You have heart diease\"\n",
        "  if pred[0]==0:\n",
        "    s=\"You don't have heart diease\"\n",
        "  return s\n"
      ],
      "metadata": {
        "id": "C_nrU1oMFlr1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_heart_disease_model=input_pred_heart()\n",
        "output_heart_disease_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "MOfjngVrF3b6",
        "outputId": "dc5d4fd3-5ef4-46f8-f382-7cf2c7df3ca1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 65\n",
            "sex: male\n",
            "chest pain: 4\n",
            "resting_bp_s: 140\n",
            "cholesterol: 306\n",
            "fasting blood sugar: 1\n",
            "resting ecg: 0\n",
            "max heart rate: 87\n",
            "exercise angina: 1\n",
            "oldpeak: 1.5\n",
            "ST slope: 2\n",
            "(1, 11)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You have heart diease'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ChatBot Model"
      ],
      "metadata": {
        "id": "jbZ1BxEiMEYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Find intents.json , words.pkl , classes.pkl , chatbotmodel.h5\n",
        "in Chatbot folder to load them </h1>"
      ],
      "metadata": {
        "id": "697xPAJCS6hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "intents = json.loads(open(\"/content/intents.json\").read()) ## upload intents.json and put the path here\n",
        "words = pickle.load(open('/content/words.pkl', 'rb')) ## upload words.pkl and put the path here\n",
        "classes = pickle.load(open('/content/classes.pkl', 'rb')) ## upload classes.pkl and put the path here\n",
        "model_chatbot = tf.keras.models.load_model('/content/chatbotmodel.h5') ## upload chatbotmodel.h5 and put the path here\n",
        "\n",
        "def clean_up_sentences(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower())\n",
        "                      for word in sentence_words]\n",
        "    return sentence_words\n",
        "def bagw(sentence):\n",
        "\n",
        "\t# separate out words from the input sentence\n",
        "\tsentence_words = clean_up_sentences(sentence)\n",
        "\tbag = [0]*len(words)\n",
        "\tfor w in sentence_words:\n",
        "\t\tfor i, word in enumerate(words):\n",
        "\n",
        "\t\t\t# check whether the word\n",
        "\t\t\t# is present in the input as well\n",
        "\t\t\tif word == w:\n",
        "\n",
        "\t\t\t\t# as the list of words\n",
        "\t\t\t\t# created earlier.\n",
        "\t\t\t\tbag[i] = 1\n",
        "\t# return a numpy array\n",
        "\treturn np.array(bag)\n",
        "def predict_class(sentence,classes):\n",
        "    bow = bagw(sentence)\n",
        "    res = model_chatbot.predict(np.array([bow]))\n",
        "    cl =np.argmax(res,axis=1)\n",
        "    p = classes[cl[0]]\n",
        "    return p\n",
        "\n",
        "def get_response(txt,classes,intents):\n",
        "    p =predict_class(txt,classes)\n",
        "\n",
        "\n",
        "    for i in intents['intents']:\n",
        "        if p==i['tag']:\n",
        "            respond =i['responses']\n",
        "\n",
        "    return (respond)"
      ],
      "metadata": {
        "id": "85ckCJBdMKM9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tags to ask about ['Cuts','Abrasions',\n",
        " 'stings',\n",
        " 'Splinter',\n",
        " 'Sprains',\n",
        " 'Strains',\n",
        " 'Fever',\n",
        " 'Nasal Congestion',\n",
        " 'Cough',\n",
        " 'Sore Throat',\n",
        " 'Gastrointestinal problems',\n",
        " 'Skin problems',\n",
        " 'Abdonominal Pain',\n",
        " 'Bruises',\n",
        " 'Broken Toe',\n",
        " 'Choking',\n",
        " 'Wound',\n",
        " 'Diarrhea',\n",
        " 'Frost bite',\n",
        " 'Heat Exhaustion',\n",
        " 'Heat Stroke',\n",
        " 'Insect Bites',\n",
        " 'nose bleed',\n",
        " 'Pulled Muscle',\n",
        " 'Rectal bleeding',\n",
        " 'Sun Burn',\n",
        " 'Testicle Pain',\n",
        " 'Vertigo',\n",
        " 'Normal Bleeding',\n",
        " 'Eye Injury',\n",
        " 'Chemical Burn',\n",
        " 'Poison',\n",
        " 'Teeth',\n",
        " 'seizure',\n",
        " 'Head Injury',\n",
        " 'Fainting',\n",
        " 'Headache',\n",
        " 'Cold',\n",
        " 'Rash',\n",
        " 'snake bite',\n",
        " 'animal bite',\n",
        " 'Drowning',\n",
        " 'CPR',\n",
        " 'Fracture']\n",
        "\n",
        " **Question Example:** What to do if someone is Drowning?,\n",
        " Do Abrasions cause scars?\n",
        " what to do if i get a strain?\n",
        " Which medicine to take if I got pulled muscle?"
      ],
      "metadata": {
        "id": "vLC6kCmXN0oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  txt=input('chat to Me: ')\n",
        "  res = get_response(txt,classes,intents)\n",
        "  print(res[0]+'\\n')"
      ],
      "metadata": {
        "id": "aiAnMeVvM7Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Covid 19 Detector\n"
      ],
      "metadata": {
        "id": "Y325PFVcPxvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Find detection_model.h5\n",
        "in Covid 19 folder to load it </h1>"
      ],
      "metadata": {
        "id": "tjdAxCBqTN3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_detect=['Covid', 'Normal', 'Viral Pneumonia']\n",
        "model_detection = tf.keras.models.load_model('detection_model.h5') ## upload detection_model.h5 and put the path here\n",
        "def predict_transfer_detect(img_path,classes,model):\n",
        "  r=tf.keras.layers.Resizing(256,256)\n",
        "  read_im=plt.imread(img_path)\n",
        "  out=r(read_im)\n",
        "  p=np.argmax(model.predict(np.expand_dims(out,axis=0)),axis=1)\n",
        "  val=classes[p[0]]\n",
        "  return val"
      ],
      "metadata": {
        "id": "3Xz5dh_OP2EN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_path='/content/0120.jpg' ###upload an image and put the path here\n",
        "output = predict_transfer_detect(im_path,all_detect,model_detection)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Jc6OWxMjQRvc",
        "outputId": "eb407f0b-99b0-438a-be9a-eff99a689768"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Covid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}